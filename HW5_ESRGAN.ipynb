{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5-ESRGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMeTh9sLUXNa",
        "outputId": "0bebb472-4312-4826-f57b-459df000de91"
      },
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDkyp9IfWMh2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.vgg import vgg19\n",
        "from torch.optim.adam import Adam\n",
        "from torchvision.utils import save_image\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "\n",
        "import time\n",
        "import os\n",
        "from random import seed\n",
        "import math\n",
        "from util import *\n",
        "\n",
        "seed(11785)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvo4JYL4zNX0"
      },
      "source": [
        "# define G, D, RRDB, perceptual loss\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, nf, gc=32, res_scale=0.2):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(nf + 0 * gc, gc, 3, padding=1, bias=True), nn.LeakyReLU())\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(nf + 1 * gc, gc, 3, padding=1, bias=True), nn.LeakyReLU())\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(nf + 2 * gc, gc, 3, padding=1, bias=True), nn.LeakyReLU())\n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(nf + 3 * gc, gc, 3, padding=1, bias=True), nn.LeakyReLU())\n",
        "        self.layer5 = nn.Sequential(nn.Conv2d(nf + 4 * gc, nf, 3, padding=1, bias=True), nn.LeakyReLU())\n",
        "\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        layer1 = self.layer1(x)\n",
        "        layer2 = self.layer2(torch.cat((x, layer1), 1))\n",
        "        layer3 = self.layer3(torch.cat((x, layer1, layer2), 1))\n",
        "        layer4 = self.layer4(torch.cat((x, layer1, layer2, layer3), 1))\n",
        "        layer5 = self.layer5(torch.cat((x, layer1, layer2, layer3, layer4), 1))\n",
        "        return layer5.mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    def __init__(self, nf, gc=32, res_scale=0.2):\n",
        "        super(RRDB, self).__init__()\n",
        "        self.layers = nn.Sequential(ResidualBlock(nf, gc),\n",
        "                                    ResidualBlock(nf, gc),\n",
        "                                    ResidualBlock(nf, gc, ))\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out.mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "def Upsamlper(nf, scale_factor=2):\n",
        "    layers = []\n",
        "    for _ in range(scale_factor//2):\n",
        "        layers += [\n",
        "            nn.Conv2d(nf, nf * (2 ** 2), 1),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.ReLU()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_conv_block=4):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        out_channels = 64\n",
        "        for _ in range(num_conv_block):\n",
        "            layers += [nn.ReflectionPad2d(1),\n",
        "                      nn.Conv2d(in_channels, out_channels, 3),\n",
        "                      nn.LeakyReLU(),\n",
        "                      nn.BatchNorm2d(out_channels)]\n",
        "            in_channels = out_channels\n",
        "\n",
        "            layers += [nn.ReflectionPad2d(1),\n",
        "                      nn.Conv2d(in_channels, out_channels, 3, 2),\n",
        "                      nn.LeakyReLU()]\n",
        "            out_channels *= 2\n",
        "\n",
        "        out_channels //= 2\n",
        "        in_channels = out_channels\n",
        "\n",
        "        layers += [nn.Conv2d(in_channels, out_channels, 3),\n",
        "                  nn.LeakyReLU(0.3),\n",
        "                  nn.Conv2d(out_channels, out_channels, 3)]\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(8192, 100),\n",
        "            nn.Linear(100, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nf=64, gc=32, scale_factor=4, n_basic_block=23):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.ReflectionPad2d(1), nn.Conv2d(in_channels, nf, 3), nn.ReLU())\n",
        "\n",
        "        basic_block_layer = []\n",
        "\n",
        "        for _ in range(n_basic_block):\n",
        "            basic_block_layer += [RRDB(nf, gc)]\n",
        "\n",
        "        self.basic_block = nn.Sequential(*basic_block_layer)\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.ReflectionPad2d(1), nn.Conv2d(nf, nf, 3), nn.ReLU())\n",
        "        self.upsample = Upsamlper(nf, scale_factor=scale_factor)\n",
        "        self.conv3 = nn.Sequential(nn.ReflectionPad2d(1), \n",
        "                                   nn.Conv2d(nf, nf, 3), \n",
        "                                   nn.ReLU(),\n",
        "                                   nn.ReflectionPad2d(1), \n",
        "                                   nn.Conv2d(nf, out_channels, 3), \n",
        "                                   nn.ReLU())\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x = self.basic_block(x1)\n",
        "        x = self.conv2(x)\n",
        "        x = self.upsample(x + x1)\n",
        "        x = self.conv3(x)\n",
        "        return x\n",
        "        \n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "\n",
        "        vgg = vgg19(pretrained=True)\n",
        "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
        "        for p in loss_network.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.network = loss_network\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, high_resolution, fake_high_resolution):\n",
        "        perception_loss = self.loss(self.network(fake_high_resolution), self.network(high_resolution))\n",
        "        return perception_loss"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6W_kXEEVb7U"
      },
      "source": [
        "# define train() and validate() in the model\n",
        "class GAN_model:\n",
        "    def __init__(self, config, train_loader=None, dev_loader=None, test_loader=None):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.num_epoch = config.num_epoch\n",
        "        self.epoch = config.load_model_epoch\n",
        "        self.image_size = config.image_size\n",
        "        self.train_loader = train_loader\n",
        "        self.dev_loader = dev_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.checkpoint_dir = config.checkpoint_dir\n",
        "        self.batch_size = config.batch_size\n",
        "        self.sample_dir = config.sample_dir\n",
        "        self.scale_factor = config.scale_factor\n",
        "\n",
        "        self.psnr_lr = config.psnr_lr\n",
        "        self.lr = config.lr\n",
        "        self.content_loss_factor = config.content_loss_factor\n",
        "        self.perceptual_loss_factor = config.perceptual_loss_factor\n",
        "        self.adversarial_loss_factor = config.adversarial_loss_factor\n",
        "\n",
        "        self.generator = Generator(3, 3, 64, scale_factor=self.scale_factor).to(self.device)\n",
        "        self.discriminator = Discriminator().to(self.device)\n",
        "\n",
        "        if config.load_model_epoch:\n",
        "          self.load_model()\n",
        "\n",
        "        psnr_epoch_indices = math.floor(config.psnr_epochs // 4)\n",
        "        interval_epoch = math.ceil(self.num_epoch // 8)\n",
        "        temp = [psnr_epoch_indices, psnr_epoch_indices * 2, psnr_epoch_indices * 4, psnr_epoch_indices * 6]\n",
        "        epoch_indices = [interval_epoch, interval_epoch * 2, interval_epoch * 4, interval_epoch * 6]\n",
        "        self.optimizer_psnr = Adam(self.generator.parameters(), lr=self.psnr_lr, betas=(config.b1, config.b2))\n",
        "        self.optimizer_generator = Adam(self.generator.parameters(), lr=self.lr, betas=(config.b1, config.b2),  # 这里为啥lr和betas是列表\n",
        "                                        weight_decay=config.weight_decay)\n",
        "        self.optimizer_discriminator = Adam(self.discriminator.parameters(), lr=self.lr, betas=(config.b1, config.b2),\n",
        "                                            weight_decay=config.weight_decay)\n",
        "\n",
        "        self.lr_scheduler_psnr = torch.optim.lr_scheduler.MultiStepLR(self.optimizer_psnr, temp, 0.5)\n",
        "        self.lr_scheduler_generator = torch.optim.lr_scheduler.MultiStepLR(self.optimizer_generator, epoch_indices, 0.5)\n",
        "        self.lr_scheduler_discriminator = torch.optim.lr_scheduler.MultiStepLR(self.optimizer_discriminator, epoch_indices, 0.5)\n",
        "\n",
        "        self.adversarial_criterion = nn.BCEWithLogitsLoss().to(self.device)\n",
        "        self.content_criterion = nn.L1Loss().to(self.device)\n",
        "        self.perception_criterion = PerceptualLoss().to(self.device)\n",
        "\n",
        "    def load_model(self):\n",
        "        %cd ./gdrive/My Drive/11785/HW5/\n",
        "        print(f\"[*] Load model from google drive {self.checkpoint_dir} epoch {self.epoch}\")\n",
        "        if not os.path.exists(self.checkpoint_dir):\n",
        "            self.makedirs = os.makedirs(self.checkpoint_dir)\n",
        "\n",
        "        if not os.listdir(self.checkpoint_dir):\n",
        "            print(f\"[!] No checkpoint in {self.checkpoint_dir}\")\n",
        "            %cd /content\n",
        "            return\n",
        "\n",
        "        generator = glob(os.path.join(self.checkpoint_dir, f'generator_{self.epoch}.pth'))\n",
        "        discriminator = glob(os.path.join(self.checkpoint_dir, f'discriminator_{self.epoch}.pth'))\n",
        "\n",
        "        if not generator:\n",
        "            print(f\"[!] No checkpoint in epoch {self.epoch - 1}\")\n",
        "            %cd /content\n",
        "            return\n",
        "\n",
        "        self.generator.load_state_dict(torch.load(generator[0]))\n",
        "        self.discriminator.load_state_dict(torch.load(discriminator[0]))\n",
        "\n",
        "        %cd /content\n",
        "\n",
        "    def pre_train_psnr(self):\n",
        "        total_step = len(self.train_loader)\n",
        "        total_G_loss, total_D_loss = 0, 0\n",
        "        self.generator.train()\n",
        "\n",
        "        if not os.path.exists(os.path.join(self.sample_dir, str(epoch))):\n",
        "            os.makedirs(os.path.join(self.sample_dir, str(epoch)))\n",
        "\n",
        "        for step, image in enumerate(self.train_loader):\n",
        "            # if step == 5: break\n",
        "            low_resolution = image['lr'].to(self.device)\n",
        "            high_resolution = image['hr'].to(self.device)\n",
        "            # low.shape = (batch_size, n_channel, 32, 32), high.shape = (batch_size, n_channel, 128, 128)\n",
        "\n",
        "            real_labels = torch.ones((high_resolution.size(0), 1)).to(self.device)\n",
        "            fake_labels = torch.zeros((high_resolution.size(0), 1)).to(self.device)\n",
        "\n",
        "            # pretrain generator\n",
        "            self.optimizer_psnr.zero_grad()\n",
        "            fake_high_resolution = self.generator(low_resolution)\n",
        "\n",
        "            generator_loss = nn.MSELoss(fake_high_resolution, high_resolution)\n",
        "\n",
        "            generator_loss.backward()\n",
        "            self.optimizer_psnr.step()\n",
        "\n",
        "            total_G_loss += generator_loss.item()\n",
        "        self.lr_scheduler_psnr.step()\n",
        "\n",
        "        # %cd ./gdrive/My Drive/11785/HW5/\n",
        "        # torch.save(self.generator.state_dict(), os.path.join(self.checkpoint_dir, f\"generator_{epoch}.pth\")\n",
        "        # %cd /content\n",
        "        return total_G_loss/total_step\n",
        "\n",
        "\n",
        "    def train(self, epoch):\n",
        "        total_step = len(self.train_loader)\n",
        "        total_G_loss, total_D_loss = 0, 0\n",
        "        self.generator.train()\n",
        "        self.discriminator.train()\n",
        "\n",
        "        for step, image in enumerate(self.train_loader):\n",
        "            # if step == 5: break\n",
        "            low_resolution = image['lr'].to(self.device)\n",
        "            high_resolution = image['hr'].to(self.device)\n",
        "            # low.shape = (batch_size, n_channel, 32, 32), high.shape = (batch_size, n_channel, 128, 128)\n",
        "\n",
        "            real_labels = torch.ones((high_resolution.size(0), 1)).to(self.device)\n",
        "            fake_labels = torch.zeros((high_resolution.size(0), 1)).to(self.device)\n",
        "\n",
        "            #generator\n",
        "            self.optimizer_generator.zero_grad()\n",
        "            fake_high_resolution = self.generator(low_resolution)\n",
        "\n",
        "            fake_prob, real_prob = self.discriminator(high_resolution), self.discriminator(fake_high_resolution)\n",
        "\n",
        "            adversarial_loss_rf = self.adversarial_criterion(real_prob - fake_prob.mean(), fake_labels)\n",
        "            adversarial_loss_fr = self.adversarial_criterion(fake_prob - real_prob.mean(), real_labels)\n",
        "            adversarial_loss = (adversarial_loss_fr + adversarial_loss_rf) / 2\n",
        "\n",
        "            perceptual_loss = self.perception_criterion(high_resolution, fake_high_resolution)\n",
        "            content_loss = self.content_criterion(fake_high_resolution, high_resolution)\n",
        "\n",
        "            generator_loss = adversarial_loss * self.adversarial_loss_factor + \\\n",
        "                              perceptual_loss * self.perceptual_loss_factor + \\\n",
        "                              content_loss * self.content_loss_factor\n",
        "\n",
        "            generator_loss.backward()\n",
        "            self.optimizer_generator.step()\n",
        "\n",
        "            #discriminator\n",
        "            self.optimizer_discriminator.zero_grad()\n",
        "\n",
        "            fake_prob, real_prob = self.discriminator(high_resolution), self.discriminator(fake_high_resolution.detach())\n",
        "\n",
        "            adversarial_loss_rf = self.adversarial_criterion(real_prob - fake_prob.mean(), real_labels)\n",
        "            adversarial_loss_fr = self.adversarial_criterion(fake_prob - real_prob.mean(), fake_labels)\n",
        "            discriminator_loss = (adversarial_loss_fr + adversarial_loss_rf) / 2\n",
        "\n",
        "            discriminator_loss.backward()\n",
        "            self.optimizer_discriminator.step()\n",
        "\n",
        "            if step == 0:\n",
        "                low_resolution_upscale = F.interpolate(low_resolution, size=128)  # 注意这里的128！\n",
        "                result = torch.cat((high_resolution, fake_high_resolution), 2)\n",
        "                result = torch.cat((result, low_resolution_upscale), 2)\n",
        "                save_image(result, os.path.join(self.sample_dir, f\"SR_{epoch}.png\"))\n",
        "            \n",
        "            total_G_loss += generator_loss.item()\n",
        "            total_D_loss += discriminator_loss.item()\n",
        "\n",
        "        self.lr_scheduler_generator.step()\n",
        "        self.lr_scheduler_discriminator.step()\n",
        "\n",
        "        %cd ./gdrive/My Drive/11785/HW5/\n",
        "        torch.save(self.generator.state_dict(), os.path.join(self.checkpoint_dir, f\"generator_{epoch}.pth\"))\n",
        "        torch.save(self.discriminator.state_dict(), os.path.join(self.checkpoint_dir, f\"discriminator_{epoch}.pth\"))\n",
        "        %cd /content\n",
        "\n",
        "        return total_G_loss/total_step, total_D_loss/total_step\n",
        "\n",
        "    def validate(self):\n",
        "      with torch.no_grad():\n",
        "        total_step = len(self.dev_loader)\n",
        "        self.generator.eval()\n",
        "        self.discriminator.eval()\n",
        "\n",
        "        total_G_loss, total_D_loss = 0, 0\n",
        "        PSNR, SSIM = 0, 0\n",
        "        for step, image in enumerate(self.dev_loader):\n",
        "          # if step == 5: break\n",
        "          low_resolution = image['lr'].to(self.device)\n",
        "          high_resolution = image['hr'].to(self.device)\n",
        "          # low.shape = (batch_size, n_channel, 32, 32), high.shape = (batch_size, n_channel, 128, 128)\n",
        "\n",
        "          real_labels = torch.ones((high_resolution.size(0), 1)).to(self.device)\n",
        "          fake_labels = torch.zeros((high_resolution.size(0), 1)).to(self.device)\n",
        "\n",
        "          #generator\n",
        "          self.optimizer_generator.zero_grad()\n",
        "          fake_high_resolution = self.generator(low_resolution)\n",
        "\n",
        "          fake_prob, real_prob = self.discriminator(high_resolution), self.discriminator(fake_high_resolution)\n",
        "\n",
        "          adversarial_loss_rf = self.adversarial_criterion(real_prob - fake_prob.mean(), fake_labels)\n",
        "          adversarial_loss_fr = self.adversarial_criterion(fake_prob - real_prob.mean(), real_labels)\n",
        "          adversarial_loss = (adversarial_loss_fr + adversarial_loss_rf) / 2\n",
        "\n",
        "          perceptual_loss = self.perception_criterion(high_resolution, fake_high_resolution)\n",
        "          content_loss = self.content_criterion(fake_high_resolution, high_resolution)\n",
        "\n",
        "          generator_loss = adversarial_loss * self.adversarial_loss_factor + \\\n",
        "                            perceptual_loss * self.perceptual_loss_factor + \\\n",
        "                            content_loss * self.content_loss_factor\n",
        "          self.optimizer_generator.step()\n",
        "\n",
        "          #discriminator\n",
        "          self.optimizer_discriminator.zero_grad()\n",
        "\n",
        "          fake_prob, real_prob = self.discriminator(high_resolution), self.discriminator(fake_high_resolution.detach())\n",
        "\n",
        "          adversarial_loss_rf = self.adversarial_criterion(real_prob - fake_prob.mean(), real_labels)\n",
        "          adversarial_loss_fr = self.adversarial_criterion(fake_prob - real_prob.mean(), fake_labels)\n",
        "          discriminator_loss = (adversarial_loss_fr + adversarial_loss_rf) / 2\n",
        "\n",
        "          total_G_loss += generator_loss.item()\n",
        "          total_D_loss += discriminator_loss.item()\n",
        "\n",
        "        # calculate PSNR and SSIM\n",
        "          high_resolution = high_resolution.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "          fake_high_resolution = fake_high_resolution.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "\n",
        "          PSNR += peak_signal_noise_ratio(high_resolution[0], fake_high_resolution[0])\n",
        "          SSIM += structural_similarity(high_resolution[0], fake_high_resolution[0], multichannel=True)\n",
        "\n",
        "        return total_G_loss/total_step, total_D_loss/total_step, PSNR/total_step, SSIM/total_step"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT_XWB3WVVR0"
      },
      "source": [
        "# parameter\n",
        "class args():\n",
        "  image_size = 128, # the height / width of the hr image to network\n",
        "  batch_size = 16\n",
        "  sample_batch_size = 1  # batch size for generating image\n",
        "  num_epoch = 60  # number of epochs to train for\n",
        "  psnr_epochs = 10\n",
        "  load_model_epoch = False  # epochs in current train (load model), false then start from scratch\n",
        "  checkpoint_dir = 'checkpoints' #path to saved models\n",
        "  sample_dir = 'samples'  #folder to output images and model checkpoints\n",
        "  scale_factor = 4 # scale factor for super resolution\n",
        "  # nf = 32 #number of filter in esrgan\n",
        "  b1 = 0.9  # coefficients used for computing running averages of gradient and its square\n",
        "  b2 = 0.999  #coefficients used for computing running averages of gradient and its square\n",
        "  weight_decay = 1e-2\n",
        "\n",
        "  psnr_lr = 2e-4\n",
        "  lr = 1e-4 #learning rate when when training generator oriented\n",
        "  content_loss_factor = 1e-1  # content loss factor when training generator oriented\n",
        "  perceptual_loss_factor = 1\n",
        "  adversarial_loss_factor = 5e-3\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvuG07UnUm4m"
      },
      "source": [
        "#download the dataset and preprocess the data (crop, resize)\n",
        "download_dataset()\n",
        "\n",
        "print('[!] Making Patches')\n",
        "crop_image('train_hr', 'train_lr', image_size, image_size/4)\n",
        "resize_image('valid_hr', image_size)\n",
        "resize_image('valid_lr', image_size/4)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o5uuOELSUoFR",
        "outputId": "258294e7-3c9a-4219-8a19-cb7d01e4338e"
      },
      "source": [
        "# 主流程，跑全部，存模型\n",
        "%cd /content\n",
        "# bmake directory not existed\n",
        "if args.checkpoint_dir is None:\n",
        "    args.checkpoint_dir = 'checkpoints'\n",
        "if not os.path.exists(args.checkpoint_dir):\n",
        "    os.makedirs(args.checkpoint_dir)\n",
        "if not os.path.exists(args.sample_dir):\n",
        "    os.makedirs(args.sample_dir)\n",
        "\n",
        "print(f\"ESRGAN start\")\n",
        "\n",
        "train_dataset = Datasets('train')\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "dev_dataset = Datasets('valid')\n",
        "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset, batch_size=args.sample_batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "model = GAN_model(args, train_loader, dev_loader)\n",
        "for epoch in range(0, args.num_epoch):\n",
        "  start_time = time.time()\n",
        "  G_loss, D_loss = model.train(epoch)\n",
        "  end_time = time.time()\n",
        "\n",
        "  print('-----------Epoch %d Training Time-------------' % epoch)\n",
        "  print(f\"[D loss {D_loss:.4f}] [G loss {G_loss:.4f}]\"\n",
        "        f\"[Duration {(end_time-start_time)/60:.4f}]\"\n",
        "        f\"\")\n",
        "  print('\\n')\n",
        "\n",
        "  start_time = time.time()\n",
        "  G_loss, D_loss, PSNR, SSIM = model.validate()\n",
        "  end_time = time.time()\n",
        "  print('-----------Epoch %d Evaluation Time-------------' % epoch)\n",
        "  print(f\"[D loss {D_loss:.4f}] [G loss {G_loss:.4f}]\"\n",
        "        f\"[PSNR {PSNR:.4f}]\"\n",
        "        f\"[SSIM {SSIM:.4f}]\"\n",
        "        f\"[Duration {(end_time-start_time)/60:.4f}]\"\n",
        "        f\"\")\n",
        "  print('\\n')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "ESRGAN start\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 0 Training Time-------------\n",
            "[D loss 0.0039] [G loss 0.0331][Duration 0.0575]\n",
            "\n",
            "\n",
            "-----------Epoch 0 Evaluation Time-------------\n",
            "[D loss 0.0347] [G loss 0.5680][PSNR 0.5277][SSIM 0.0060][Duration 0.0087]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 1 Training Time-------------\n",
            "[D loss 0.0011] [G loss 0.0405][Duration 0.0561]\n",
            "\n",
            "\n",
            "-----------Epoch 1 Evaluation Time-------------\n",
            "[D loss 0.0107] [G loss 0.8504][PSNR 0.3733][SSIM 0.0036][Duration 0.0086]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 2 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0365][Duration 0.0568]\n",
            "\n",
            "\n",
            "-----------Epoch 2 Evaluation Time-------------\n",
            "[D loss 0.0000] [G loss 0.8803][PSNR 0.3701][SSIM 0.0039][Duration 0.0102]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 3 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0375][Duration 0.0571]\n",
            "\n",
            "\n",
            "-----------Epoch 3 Evaluation Time-------------\n",
            "[D loss 0.0000] [G loss 0.6608][PSNR 0.3745][SSIM 0.0041][Duration 0.0086]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 4 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0323][Duration 0.0565]\n",
            "\n",
            "\n",
            "-----------Epoch 4 Evaluation Time-------------\n",
            "[D loss 0.0000] [G loss 0.7311][PSNR 0.3189][SSIM 0.0035][Duration 0.0087]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 5 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0368][Duration 0.0577]\n",
            "\n",
            "\n",
            "-----------Epoch 5 Evaluation Time-------------\n",
            "[D loss 0.1109] [G loss 0.9856][PSNR 0.3438][SSIM 0.0033][Duration 0.0083]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 6 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0414][Duration 0.0573]\n",
            "\n",
            "\n",
            "-----------Epoch 6 Evaluation Time-------------\n",
            "[D loss 0.1552] [G loss 0.9437][PSNR 0.3664][SSIM 0.0047][Duration 0.0083]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 7 Training Time-------------\n",
            "[D loss 0.0012] [G loss 0.0372][Duration 0.0561]\n",
            "\n",
            "\n",
            "-----------Epoch 7 Evaluation Time-------------\n",
            "[D loss 0.0000] [G loss 0.8116][PSNR 0.3895][SSIM 0.0053][Duration 0.0084]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 8 Training Time-------------\n",
            "[D loss 0.0003] [G loss 0.0295][Duration 0.0583]\n",
            "\n",
            "\n",
            "-----------Epoch 8 Evaluation Time-------------\n",
            "[D loss 0.0000] [G loss 0.8203][PSNR 0.3239][SSIM 0.0048][Duration 0.0085]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 9 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0335][Duration 0.0572]\n",
            "\n",
            "\n",
            "-----------Epoch 9 Evaluation Time-------------\n",
            "[D loss 0.0217] [G loss 0.6849][PSNR 0.3853][SSIM 0.0057][Duration 0.0087]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 10 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0298][Duration 0.0575]\n",
            "\n",
            "\n",
            "-----------Epoch 10 Evaluation Time-------------\n",
            "[D loss 0.5588] [G loss 0.5647][PSNR 0.3758][SSIM 0.0063][Duration 0.0096]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 11 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0296][Duration 0.0564]\n",
            "\n",
            "\n",
            "-----------Epoch 11 Evaluation Time-------------\n",
            "[D loss 1.3426] [G loss 0.6368][PSNR 0.3403][SSIM 0.0069][Duration 0.0086]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 12 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0249][Duration 0.0563]\n",
            "\n",
            "\n",
            "-----------Epoch 12 Evaluation Time-------------\n",
            "[D loss 1.9596] [G loss 1.0307][PSNR 0.3971][SSIM 0.0055][Duration 0.0087]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 13 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0346][Duration 0.0562]\n",
            "\n",
            "\n",
            "-----------Epoch 13 Evaluation Time-------------\n",
            "[D loss 1.8989] [G loss 0.5668][PSNR 0.3866][SSIM 0.0059][Duration 0.0087]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 14 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0309][Duration 0.0568]\n",
            "\n",
            "\n",
            "-----------Epoch 14 Evaluation Time-------------\n",
            "[D loss 1.7996] [G loss 0.5434][PSNR 0.3959][SSIM 0.0066][Duration 0.0086]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 15 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0293][Duration 0.0563]\n",
            "\n",
            "\n",
            "-----------Epoch 15 Evaluation Time-------------\n",
            "[D loss 1.6832] [G loss 0.4432][PSNR 0.4357][SSIM 0.0083][Duration 0.0087]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 16 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0324][Duration 0.0710]\n",
            "\n",
            "\n",
            "-----------Epoch 16 Evaluation Time-------------\n",
            "[D loss 1.6981] [G loss 0.6425][PSNR 0.4144][SSIM 0.0062][Duration 0.0087]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 17 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0308][Duration 0.1060]\n",
            "\n",
            "\n",
            "-----------Epoch 17 Evaluation Time-------------\n",
            "[D loss 1.6625] [G loss 0.5541][PSNR 0.3658][SSIM 0.0075][Duration 0.0084]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 18 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0263][Duration 0.0859]\n",
            "\n",
            "\n",
            "-----------Epoch 18 Evaluation Time-------------\n",
            "[D loss 2.1162] [G loss 0.6808][PSNR 0.3926][SSIM 0.0067][Duration 0.0087]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 19 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0245][Duration 0.0873]\n",
            "\n",
            "\n",
            "-----------Epoch 19 Evaluation Time-------------\n",
            "[D loss 1.9187] [G loss 0.6167][PSNR 0.4738][SSIM 0.0079][Duration 0.0083]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 20 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0279][Duration 0.1116]\n",
            "\n",
            "\n",
            "-----------Epoch 20 Evaluation Time-------------\n",
            "[D loss 2.0631] [G loss 0.8468][PSNR 0.5281][SSIM 0.0065][Duration 0.0088]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 21 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0260][Duration 0.0901]\n",
            "\n",
            "\n",
            "-----------Epoch 21 Evaluation Time-------------\n",
            "[D loss 1.6884] [G loss 0.5367][PSNR 0.4473][SSIM 0.0102][Duration 0.0086]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 22 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0346][Duration 0.0793]\n",
            "\n",
            "\n",
            "-----------Epoch 22 Evaluation Time-------------\n",
            "[D loss 1.5967] [G loss 0.5742][PSNR 0.5283][SSIM 0.0084][Duration 0.0085]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 23 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0254][Duration 0.0811]\n",
            "\n",
            "\n",
            "-----------Epoch 23 Evaluation Time-------------\n",
            "[D loss 1.4751] [G loss 0.8320][PSNR 0.4634][SSIM 0.0065][Duration 0.0083]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 24 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0246][Duration 0.0878]\n",
            "\n",
            "\n",
            "-----------Epoch 24 Evaluation Time-------------\n",
            "[D loss 1.1947] [G loss 0.3467][PSNR 0.4168][SSIM 0.0066][Duration 0.0101]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 25 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0276][Duration 0.0917]\n",
            "\n",
            "\n",
            "-----------Epoch 25 Evaluation Time-------------\n",
            "[D loss 0.9595] [G loss 0.4977][PSNR 0.5890][SSIM 0.0109][Duration 0.0090]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 26 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0271][Duration 0.0961]\n",
            "\n",
            "\n",
            "-----------Epoch 26 Evaluation Time-------------\n",
            "[D loss 1.4456] [G loss 0.4765][PSNR 0.5505][SSIM 0.0114][Duration 0.0084]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 27 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0267][Duration 0.0886]\n",
            "\n",
            "\n",
            "-----------Epoch 27 Evaluation Time-------------\n",
            "[D loss 1.0107] [G loss 0.8025][PSNR 0.4416][SSIM 0.0067][Duration 0.0085]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 28 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0264][Duration 0.0890]\n",
            "\n",
            "\n",
            "-----------Epoch 28 Evaluation Time-------------\n",
            "[D loss 1.0944] [G loss 0.7323][PSNR 0.3619][SSIM 0.0082][Duration 0.0114]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 29 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0288][Duration 0.0852]\n",
            "\n",
            "\n",
            "-----------Epoch 29 Evaluation Time-------------\n",
            "[D loss 0.8308] [G loss 0.5462][PSNR 0.4476][SSIM 0.0098][Duration 0.0084]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 30 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0246][Duration 0.1021]\n",
            "\n",
            "\n",
            "-----------Epoch 30 Evaluation Time-------------\n",
            "[D loss 1.1381] [G loss 0.6098][PSNR 0.4255][SSIM 0.0080][Duration 0.0109]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 31 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0258][Duration 0.0874]\n",
            "\n",
            "\n",
            "-----------Epoch 31 Evaluation Time-------------\n",
            "[D loss 0.8827] [G loss 0.3681][PSNR 0.4643][SSIM 0.0098][Duration 0.0084]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 32 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0281][Duration 0.0963]\n",
            "\n",
            "\n",
            "-----------Epoch 32 Evaluation Time-------------\n",
            "[D loss 1.0704] [G loss 0.4879][PSNR 0.4195][SSIM 0.0068][Duration 0.0096]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 33 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0292][Duration 0.0886]\n",
            "\n",
            "\n",
            "-----------Epoch 33 Evaluation Time-------------\n",
            "[D loss 0.9983] [G loss 0.6844][PSNR 0.3646][SSIM 0.0082][Duration 0.0086]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 34 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0257][Duration 0.0881]\n",
            "\n",
            "\n",
            "-----------Epoch 34 Evaluation Time-------------\n",
            "[D loss 0.9669] [G loss 0.4574][PSNR 0.4066][SSIM 0.0072][Duration 0.0085]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n",
            "/content\n",
            "-----------Epoch 35 Training Time-------------\n",
            "[D loss 0.0000] [G loss 0.0235][Duration 0.0858]\n",
            "\n",
            "\n",
            "-----------Epoch 35 Evaluation Time-------------\n",
            "[D loss 1.0252] [G loss 0.7418][PSNR 0.3965][SSIM 0.0072][Duration 0.0085]\n",
            "\n",
            "\n",
            "/content/gdrive/My Drive/11785/HW5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-472b925b991c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mG_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-e450963e30f7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd ./gdrive/My Drive/11785/HW5/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"generator_{epoch}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"discriminator_{epoch}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd /content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM3HIYEwNaR1"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIrhiEqVWFh-",
        "outputId": "8757f507-0a88-4b97-a1f2-bbf46d344e22"
      },
      "source": [
        "%cd /content/gdrive/My Drive/11785/HW5\n",
        "\n",
        "# change to the google drive folder to generate test image\n",
        "def test(target_folder, load_model_epoch, args, resize=False): # generate image from testset\n",
        "    if resize:\n",
        "      resize_image('test_lr/'+target_folder, resize, 'test_resize/'+target_folder)\n",
        "      test_dataset = Datasets(mode='test_resize/'+target_folder)\n",
        "    else:\n",
        "      test_dataset = Datasets(mode='test_lr/'+target_folder)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=args.sample_batch_size)\n",
        "    if not os.path.exists('test_results/'+target_folder):\n",
        "      os.makedirs('test_results/'+target_folder)\n",
        "    \n",
        "    model = GAN_model(args, test_loader=test_loader)\n",
        "    # model.to(args.device)\n",
        "    model.generator.load_state_dict(torch.load(os.path.join(args.checkpoint_dir, f'generator_{load_model_epoch}.pth')))\n",
        "    with torch.no_grad():\n",
        "      model.generator.eval()\n",
        "      for step, image in enumerate(test_loader):\n",
        "        lr = image['lr'].to(args.device)\n",
        "        image_name = image['hr']\n",
        "        outputs = model.generator(lr)\n",
        "        save_image(outputs, os.path.join('test_results/'+target_folder, image_name[0]))\n",
        "\n",
        "target_folder = ['large_test', 'small_test', 'comics', 'structures']\n",
        "load_model_epoch = 23\n",
        "for t in target_folder:\n",
        "    test(t, load_model_epoch, args, resize=384)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/11785/HW5\n",
            "[*] [0/100] Make patch test_lr/large_test/108070.png\n",
            "[*] [0/19] Make patch test_lr/small_test/barbara.png\n",
            "[*] [0/109] Make patch test_lr/comics/AisazuNihaIrarenai.png\n",
            "[*] [0/100] Make patch test_lr/structures/img_002.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20YcxzeYZoi1"
      },
      "source": [
        "TEST_LABEL_PATH = 'test_labels/small_test/'\n",
        "TEST_RESULT_PATH = 'test_results/small_test/'\n",
        "TEST_RESTORE_PATH =  'test_restore_results/small_test/'\n",
        "\n",
        "\n",
        "scores = compute_scores(TEST_LABEL_PATH, TEST_RESULT_PATH, TEST_RESTORE_PATH)\n",
        "print('small_test: ', scores)\n",
        "\n",
        "TEST_LABEL_PATH = 'test_labels/large_test/'\n",
        "TEST_RESULT_PATH = 'test_results/large_test/'\n",
        "TEST_RESTORE_PATH =  'test_restore_results/large_test/'\n",
        "\n",
        "scores = compute_scores(TEST_LABEL_PATH, TEST_RESULT_PATH, TEST_RESTORE_PATH)\n",
        "print('large_test: ', scores)\n",
        "\n",
        "TEST_LABEL_PATH = 'test_labels/comics/'\n",
        "TEST_RESULT_PATH = 'test_results/comics/'\n",
        "TEST_RESTORE_PATH =  'test_restore_results/comics/'\n",
        "\n",
        "scores = compute_scores(TEST_LABEL_PATH, TEST_RESULT_PATH, TEST_RESTORE_PATH)\n",
        "print('comic: ', scores)\n",
        "\n",
        "TEST_LABEL_PATH = 'test_labels/structures/'\n",
        "TEST_RESULT_PATH = 'test_results/structures/'\n",
        "TEST_RESTORE_PATH =  'test_restore_results/structures/'\n",
        "\n",
        "scores = compute_scores(TEST_LABEL_PATH, TEST_RESULT_PATH, TEST_RESTORE_PATH)\n",
        "print('structures: ', scores)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl9Cgcwu1Kca"
      },
      "source": [
        "\n",
        "\n",
        "# SRResnet result\n",
        "\n",
        "# without resize: 20.105424887688333 0.6729383997153978\n",
        "\n",
        "# 24: 18.83101057509526 0.5856139937502995\n",
        "\n",
        "# 96: 20.764082463704487 0.6797901165309642\n",
        "\n",
        "# 128: 20.978491100856992 0.6822266197132818\n",
        "\n",
        "# 196: 21.213560813348508 0.6941931686460572\n",
        "\n",
        "\n",
        "# ESRGAN result\n",
        "\n",
        "# 64:size\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 21.45309622385869 0.6802526439816884\n",
        "# small_test:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 20.747147799217373 0.6334000266590871\n",
        "# large_test:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 16.006150111341537 0.5802311635659595\n",
        "# comic:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 16.554238189410377 0.48245588636768943\n",
        "# structures:  None\n",
        "\n",
        "# 128:size\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 23.1183977326426 0.75362138135607\n",
        "# small_test:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 21.371081353390437 0.678602171482434\n",
        "# large_test:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 18.65251946090006 0.6674142273974711\n",
        "# comic:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 19.090746158291886 0.595484765826424\n",
        "# structures:  None\n",
        "\n",
        "# 256:size\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 23.487613735752547 0.7684400402702198\n",
        "# small_test:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 21.299195917203264 0.6826661956559363\n",
        "# large_test:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 20.69369763089257 0.7666858174277854\n",
        "# comic:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 20.81196694237977 0.7011574299367789\n",
        "# structures:  None\n",
        "\n",
        "# 384:size\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 23.53565344322597 0.7697716136107305\n",
        "# small_test:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 21.228789973878655 0.6815376732585176\n",
        "# large_test:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 21.038829295921786 0.787277712110814\n",
        "# comic:  None\n",
        "# In computer scores\n",
        "# In get test images\n",
        "# 20.976102972057376 0.704620308515809\n",
        "# structures:  None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}